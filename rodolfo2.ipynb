{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import seaborn as sns, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"dados_classificacao/dados_hospital.csv\", sep=',')\n",
    "\n",
    "# renomear = ['gender','age','race','marital_status','schooling','city','distance','cancer','day','shift','month','season','lead_time','no_cons_scheduled_previous_year','no_ns_cons_previous_year','no_exa_scheduled_previous_year','no_ns_exa_previous_year','no_show'] \n",
    "# mapeamento = {coluna: renomear[i] for i, coluna in enumerate(data.columns)}\n",
    "# # # # Renomear as colunas com o mapeamento\n",
    "# data.rename(columns=mapeamento, inplace=True)\n",
    "# data.drop(['city','season'],axis=1,inplace=True)\n",
    "# data.to_csv(\"dados_classificacao/dados_hospital.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64    16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "for cl in data.columns:\n",
    "    if data[cl].dtypes==np.object0:\n",
    "        data[cl] = le.fit_transform(data[cl])\n",
    "\n",
    "data.dtypes.value_counts()\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# encoded_data = encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "# encoded_df = pd.concat([data.drop(categorical_cols, axis=1),\n",
    "#                         pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))],\n",
    "#                        axis=1)\n",
    "# data = encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'no_show'\n",
    "X = data.drop([target,'cancer'], axis=1)\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "contingency_table = pd.crosstab(data['race'], data['no_show'])\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(\"Estatística de Teste Chi-Quadrado:\", chi2)\n",
    "print(\"Valor p:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, f_oneway\n",
    "# data[\"a2\"] = data.age*1\n",
    "grouped = data.groupby('no_show')\n",
    "# Realizar o teste t de Student para cada grupo\n",
    "group1 = grouped.get_group(1)['distance']\n",
    "group2 = grouped.get_group(0)['distance']\n",
    "t_statistic, p_value = ttest_ind(group1, group2)\n",
    "# Imprimir os resultados\n",
    "print(\"Estatística t:\", t_statistic)\n",
    "print(\"Valor p:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group1.mean(),group1.std(),group2.mean(),group2.std(),len(group1),len(group2)\n",
    "percentiles = np.linspace(1e-10,1.0-1.e-10,10)\n",
    "variable = 'no_exa_scheduled_previous_year'\n",
    "results = {}\n",
    "for percentile in percentiles:\n",
    "    # Calcular o valor do percentil para a coluna 'age'\n",
    "    age_percentile = data[variable].quantile(percentile)\n",
    "    \n",
    "    # Filtrar o DataFrame com base no percentil\n",
    "    filtered_df = data[data[variable] <= age_percentile]\n",
    "    \n",
    "    # Calcular a proporção de 'no_show' no subconjunto filtrado\n",
    "    no_show_proportion = filtered_df['no_show'].value_counts(normalize=True)\n",
    "    \n",
    "    # Armazenar a proporção no dicionário de resultados\n",
    "    results[age_percentile] = no_show_proportion\n",
    "\n",
    "# Criar um DataFrame a partir do dicionário de resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(\"Proporções de 'no_show' para os percentis:\")\n",
    "results_df\n",
    "results_df.loc[0,:].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(data.columns)\n",
    "sns.set(style = 'ticks', color_codes=True)\n",
    "sns.pairplot(data=data, vars = data[[variables[i] for i in [1,5,10]]] , hue = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x='no_show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(data= data.corr(), annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,2, figsize=(16,16))\n",
    "sns.distplot(data.var_1, bins = 20, ax=ax[0,0]) \n",
    "sns.distplot(data.var_2, bins = 20, ax=ax[0,1]) \n",
    "sns.distplot(data.var_3, bins = 20, ax=ax[1,0]) \n",
    "sns.distplot(data.var_4, bins = 20, ax=ax[1,1]) \n",
    "sns.distplot(data.var_5, bins = 20, ax=ax[2,0])\n",
    "sns.distplot(data.var_6, bins = 20, ax=ax[2,1])\n",
    "sns.distplot(data.var_7, bins = 20, ax=ax[3,0]) \n",
    "sns.distplot(data.var_8, bins = 20, ax=ax[3,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlations = X.corrwith(y)\n",
    "correlations.sort_values(inplace=True)\n",
    "correlations\n",
    "ax = correlations.plot(kind='bar')\n",
    "ax.set(ylim=[-.1, 0.1], ylabel='pearson correlation')\n",
    "\n",
    "# X.corrwith(data.COMPARECEU_AGENDAMENTO_EXAME).plot.bar(figsize = (15, 10), title = \"Correlation with Target\", fontsize = 10,grid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data.ESTACAO_AGENDAMENTO,y=y,hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Matrix\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>schooling</th>\n",
       "      <th>distance</th>\n",
       "      <th>cancer</th>\n",
       "      <th>day</th>\n",
       "      <th>shift</th>\n",
       "      <th>month</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>no_cons_scheduled_previous_year</th>\n",
       "      <th>no_ns_cons_previous_year</th>\n",
       "      <th>no_exa_scheduled_previous_year</th>\n",
       "      <th>no_ns_exa_previous_year</th>\n",
       "      <th>no_show</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>69.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8380</th>\n",
       "      <td>0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4196 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age  race  marital_status  schooling  distance  cancer  day  \\\n",
       "0          0  77.0   1.0               0        0.0       1.0       0    1   \n",
       "2          0  87.0   1.0               0        2.0       1.0       1    3   \n",
       "3          0  83.0   1.0               0        2.0       1.0       0    4   \n",
       "5          0  64.0   1.0               4        2.0       1.0       1    5   \n",
       "6          0  64.0   1.0               4        2.0       1.0       1    5   \n",
       "...      ...   ...   ...             ...        ...       ...     ...  ...   \n",
       "8373       0  35.0   1.0               0        2.0     122.0       0    1   \n",
       "8374       1  55.0   1.0               0        1.0     124.0       1    1   \n",
       "8376       1  36.0   1.0               0        2.0      19.0       0    3   \n",
       "8377       1  55.0   1.0               0        1.0     150.0       0    1   \n",
       "8380       0  65.0   1.0               0        1.0       1.0       1    4   \n",
       "\n",
       "      shift  month  lead_time  no_cons_scheduled_previous_year  \\\n",
       "0         0     11        6.0                              1.0   \n",
       "2         0     11      195.0                              3.0   \n",
       "3         0     11        6.0                             17.0   \n",
       "5         1      7       69.0                              9.0   \n",
       "6         0     10        6.0                              7.0   \n",
       "...     ...    ...        ...                              ...   \n",
       "8373      0      2       19.0                              0.0   \n",
       "8374      0      2        9.0                              1.0   \n",
       "8376      0      2        6.0                              1.0   \n",
       "8377      1      2        8.0                              1.0   \n",
       "8380      0      2       10.0                              1.0   \n",
       "\n",
       "      no_ns_cons_previous_year  no_exa_scheduled_previous_year  \\\n",
       "0                          0.0                             0.0   \n",
       "2                          0.0                             1.0   \n",
       "3                          1.0                             2.0   \n",
       "5                          0.0                             5.0   \n",
       "6                          0.0                             4.0   \n",
       "...                        ...                             ...   \n",
       "8373                       0.0                             0.0   \n",
       "8374                       0.0                             0.0   \n",
       "8376                       0.0                             0.0   \n",
       "8377                       0.0                             0.0   \n",
       "8380                       0.0                             0.0   \n",
       "\n",
       "      no_ns_exa_previous_year  no_show  \n",
       "0                         0.0      1.0  \n",
       "2                         0.0      1.0  \n",
       "3                         0.0      1.0  \n",
       "5                         0.0      1.0  \n",
       "6                         0.0      1.0  \n",
       "...                       ...      ...  \n",
       "8373                      0.0      1.0  \n",
       "8374                      0.0      1.0  \n",
       "8376                      0.0      1.0  \n",
       "8377                      0.0      1.0  \n",
       "8380                      0.0      1.0  \n",
       "\n",
       "[4196 rows x 16 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "data_sem_outliers = data[(data >= limite_inferior) & (data <= limite_superior)]\n",
    "data_sem_outliers.dropna(inplace=True)\n",
    "\n",
    "target = 'no_show'\n",
    "X = data_sem_outliers.drop([target,'cancer'], axis=1)\n",
    "y = data_sem_outliers[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  111]\n",
      " [   0 1566]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer,f1_score,accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "\n",
    "## remover outlier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minha_metrica(y_true, y_pred):\n",
    "    threshold = 0.5  # Defina o threshold desejado\n",
    "\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= threshold))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < threshold))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred < threshold))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= threshold))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "my_scorer = make_scorer(minha_metrica,greater_is_better=True)\n",
    "\n",
    "piplr = Pipeline([('escala',MinMaxScaler()),('modelo', LogisticRegression(penalty='l2'))])\n",
    "cvr = cross_validate(piplr,X_train, y_train,scoring = my_scorer, cv=StratifiedKFold(3),return_estimator=True)\n",
    "\n",
    "estimators = cvr['estimator']\n",
    "best_model = estimators[np.argmax(cvr['test_score'])]\n",
    "cm = confusion_matrix(y_test,best_model.predict(X_test)) \n",
    "\n",
    "print(cm)\n",
    "print(cvr['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix\n",
    "# classifier = LogisticRegression(penalty='l2')\n",
    "kf =  StratifiedKFold(n_splits=5)\n",
    "scr = []\n",
    "pipe = Pipeline([('escala',StandardScaler()),('modelo', LogisticRegression(penalty='l2'))])\n",
    "for train, test in kf.split(X,y):\n",
    "    Xtr,Xts = X.iloc[train,:],X.iloc[test,:]\n",
    "    ytr,yts = y[train],y[test]\n",
    "    pipe.fit(Xtr,ytr)\n",
    "    ypr = pipe.predict(Xts)\n",
    "    cm = confusion_matrix(yts,ypr,normalize='true')\n",
    "    scr.append(np.sqrt(cm[0][0]*cm[1][1]))\n",
    "    \n",
    "print(scr)\n",
    "    # classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "kf =  StratifiedKFold(n_splits=5)\n",
    "scr = []\n",
    "poly = PolynomialFeatures(degree=5)\n",
    "pipe = Pipeline([(('poli',poly)),('escala',StandardScaler()),('modelo', LogisticRegression(penalty='l2'))])\n",
    "for train, test in kf.split(X,y):\n",
    "    Xtr,Xts = X.iloc[train,:],X.iloc[test,:]\n",
    "    ytr,yts = y[train],y[test]\n",
    "    pipe.fit(Xtr,ytr)\n",
    "    ypr = pipe.predict(Xts)\n",
    "    cm = confusion_matrix(yts,ypr,normalize='true')\n",
    "    # scr.append(np.sqrt(cm[0][0]*cm[1][1]))\n",
    "    scr.append(accuracy_score(yts,ypr))\n",
    "    \n",
    "print(scr)\n",
    "    # classifier.fit(X_train, y_train)\n",
    "\n",
    "# scr2 = cross_val_score(pipe,X,y,cv = 5)\n",
    "# print(scr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer,f1_score,accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "\n",
    "def minha_metrica(y_true, y_pred):\n",
    "    threshold = 0.5  # Defina o threshold desejado\n",
    "    \n",
    "\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= threshold))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < threshold))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred < threshold))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= threshold))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "my_scorer = make_scorer(minha_metrica,greater_is_better=True)\n",
    "\n",
    "poly = PolynomialFeatures(degree=8)\n",
    "pipe = Pipeline([(('poli',poly)),('escala',StandardScaler()),('modelo', LogisticRegression(penalty='l2'))]) \n",
    "# pipe = Pipeline([('escala',StandardScaler()),(('poli',poly)),('modelo', LogisticRegression(penalty='l2'))])\n",
    "# pipe = Pipeline([(('poli',poly)),('escala',StandardScaler()),('modelo', Lasso(alpha = , max_iter=100000))])\n",
    "cvr = cross_validate(pipe, X_train, y_train, cv=StratifiedKFold(3), scoring=my_scorer, return_estimator=True)\n",
    "cvr['test_score']\n",
    "estimators = cvr['estimator']\n",
    "best_model = estimators[np.argmax(cvr['test_score'])]\n",
    "y_pred = best_model.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31040191, 0.31804676, 0.30912059])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer,f1_score,accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "\n",
    "\n",
    "def minha_metrica(y_true, y_pred):\n",
    "    threshold = 0.5  # Defina o threshold desejado\n",
    "    \n",
    "\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= threshold))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < threshold))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred < threshold))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= threshold))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "my_scorer = make_scorer(minha_metrica,greater_is_better=True)\n",
    "\n",
    "poly = PolynomialFeatures(degree = 6)\n",
    "pipe = Pipeline([(('poli',poly)),('escala',StandardScaler()),('modelo', LogisticRegression(penalty='l2'))]) \n",
    "# pipe = Pipeline([('escala',StandardScaler()),(('poli',poly)),('modelo', LogisticRegression(penalty='l2'))])\n",
    "# pipe = Pipeline([(('poli',poly)),('escala',StandardScaler()),('modelo', Lasso(alpha = , max_iter=100000))])\n",
    "cvr = cross_validate(pipe, X_train, y_train, cv=StratifiedKFold(3), scoring=my_scorer, return_estimator=True)\n",
    "cvr['test_score']\n",
    "# estimators = cvr['estimator']\n",
    "# best_model = estimators[np.argmax(cvr['test_score'])]\n",
    "# y_pred = best_model.predict_proba(X_test)[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "results = pd.DataFrame([['Logistic Regression (Lasso)', acc,prec,rec,f1]],columns=['Model', 'Accuracy', 'Precision', 'Recall','F1 Score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "print(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = classifier.predict_proba(X_test)\n",
    "from scipy import integrate\n",
    "def capcurve(y_values, y_preds_proba):\n",
    "    num_pos_obs = np.sum(y_values)\n",
    "    num_count = len(y_values)\n",
    "    rate_pos_obs = float(num_pos_obs) / float(num_count)\n",
    "    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n",
    "    xx = np.arange(num_count) / float(num_count - 1)\n",
    "    \n",
    "    y_cap = np.c_[y_values,y_preds_proba]\n",
    "    y_cap_df_s = pd.DataFrame(data=y_cap)\n",
    "    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n",
    "    \n",
    "    print(y_cap_df_s.head(20))\n",
    "    \n",
    "    yy = np.cumsum(y_cap_df_s[0]) / float(num_pos_obs)\n",
    "    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n",
    "    \n",
    "    percent = 0.5\n",
    "    row_index = int(np.trunc(num_count * percent))\n",
    "    \n",
    "    val_y1 = yy[row_index]\n",
    "    val_y2 = yy[row_index+1]\n",
    "    if val_y1 == val_y2:\n",
    "        val = val_y1*1.0\n",
    "    else:\n",
    "        val_x1 = xx[row_index]\n",
    "        val_x2 = xx[row_index+1]\n",
    "        val = val_y1 + ((val_x2 - percent)/(val_x2 - val_x1))*(val_y2 - val_y1)\n",
    "    \n",
    "    sigma_ideal = 1 * xx[num_pos_obs - 1 ] / 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n",
    "    sigma_model = integrate.simps(yy,xx)\n",
    "    sigma_random = integrate.simps(xx,xx)\n",
    "    \n",
    "    ar_value = (sigma_model - sigma_random) / (sigma_ideal - sigma_random)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n",
    "    ax.plot(xx,yy, color='red', label='User Model')\n",
    "    ax.plot(xx,xx, color='blue', label='Random Model')\n",
    "    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n",
    "    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n",
    "    \n",
    "    plt.xlim(0, 1.02)\n",
    "    plt.ylim(0, 1.25)\n",
    "    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n",
    "    plt.xlabel('% of the data')\n",
    "    plt.ylabel('% of positive obs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capcurve(y_test,y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator= classifier, X=X_train, y=y_train, cv=10)\n",
    "accuracies.mean()\n",
    "accuracies.std()\n",
    "print('Logistic Regression (Lasso) Accuracy: %0.3f (+/- %0.3f)' % (accuracies.mean(), accuracies.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n",
    "           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n",
    "           ],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection \n",
    "#Recursive feature elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Select best feature \n",
    "rfe = RFE(classifier, n_features_to_select= None)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "#Summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Correlation Matrix\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = X_train[X_train.columns[rfe.support_]].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0, penalty= 'l2')\n",
    "classifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n",
    "\n",
    "# Predicting Test Set\n",
    "y_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "model_results = pd.DataFrame([['Logistic Regression RFE (Lasso)', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = classifier.predict_proba(X_test[X_train.columns[rfe.support_]])\n",
    "capcurve(y_test,y_pred_prob[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
