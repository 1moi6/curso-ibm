{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 3, Part a: Logistic Regression LAB\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Introduction\n","\n","We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01) database, which was built from the recordings of study participants who carried a smartphone with an embedded inertial sensor while performing activities of daily living (ADL). The objective is to classify the activities the participants performed into one of the six following categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.\n","\n","The following information is provided for each record in the dataset:\n","\n","*   Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration\n","*   Triaxial Angular velocity from the gyroscope\n","*   A 561-feature vector with time and frequency domain variables\n","*   The activity label\n","\n","More information about the features are available on the website linked above.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn"]},{"cell_type":"code","execution_count":null,"metadata":{"run_control":{"marked":true}},"outputs":[],"source":["import seaborn as sns, pandas as pd, numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 1\n","\n","Import the data and do the following:\n","\n","*   Examine the data types--there are many columns, so it might be wise to use value counts.\n","*   Determine if the floating point values need to be scaled.\n","*   Determine the breakdown of each activity.\n","*   Encode the activity label as an integer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["### BEGIN SOLUTION\n","# data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')\n","data = pd.read_csv(\"dados_classificacao/dados.csv\", sep=',')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["The data columns are all floats except for the activity label.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mapeamento = {coluna: i+1 for i, coluna in enumerate(data.columns)}\n","# Renomear as colunas com o mapeamento\n","data.rename(columns=mapeamento, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["data.dtypes.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["data.dtypes.tail()"]},{"attachments":{},"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["The data are all scaled from -1 (minimum) to 1.0 (maximum).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Examine the breakdown of activities; they are relatively balanced.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.\\\n","Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","for cl in data.columns:\n","    if data[cl].dtypes==np.object0:\n","        data[cl] = le.fit_transform(data[cl])\n","\n","data.dtypes.value_counts()\n","### END SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data[18].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 2\n","\n","*   Calculate the correlations between the dependent variables.\n","*   Create a histogram of the correlation values.\n","*   Identify those that are most correlated (either positively or negatively).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n","# Calculate the correlation values\n","feature_cols = data.columns[:-1]\n","corr_values = data[feature_cols].corr()\n","\n","# Simplify by emptying all the data below the diagonal\n","tril_index = np.tril_indices_from(corr_values)\n","\n","# Make the unused values NaNs\n","for coord in zip(*tril_index):\n","    corr_values.iloc[coord[0], coord[1]] = np.NaN\n","    \n","# Stack the data and convert to a data frame\n","corr_values = (corr_values\n","               .stack()\n","               .to_frame()\n","               .reset_index()\n","               .rename(columns={'level_0':'feature1',\n","                                'level_1':'feature2',\n","                                0:'correlation'}))\n","\n","# Get the absolute values for sorting\n","corr_values['abs_correlation'] = corr_values.correlation.abs()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A histogram of the absolute value correlations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sns.set_context('talk')\n","sns.set_style('white')\n","\n","ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n","ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# The most highly correlated values\n","corr_values.sort_values('correlation', ascending=False).query('abs_correlation<0.25')\n","### END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 3\n","\n","*   Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n","*   Regardless of the method used to split the data, compare the ratio of classes in both the train and test splits.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Get the split indexes\n","strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n","                                          test_size=0.3, \n","                                          random_state=42)\n","\n","train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data[[18]]))\n","\n","# Create the dataframes\n","X_train = data.loc[train_idx, feature_cols]\n","y_train = data.loc[train_idx, 18]\n","\n","X_test  = data.loc[test_idx, feature_cols]\n","y_test  = data.loc[test_idx, 18]"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["y_test.value_counts(normalize=True)\n","### END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 4\n","\n","*   Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n","*   Using cross validation to determine the hyperparameters and fit models using L1 and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","X = data.drop(columns=[18])\n","y = data[18].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","# Standard logistic regression\n","lr = LogisticRegression(solver='liblinear')\n","pipe = Pipeline([('poli',PolynomialFeatures(degree=2)),('escala',StandardScaler()),('classificacao',lr)])\n","pipe.fit(X_train, y_train)\n","poli = pipe.named_steps['poli']\n","feature_names = poli.get_feature_names_out()\n","feature_names"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# L1 regularized logistic regression\n","lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear')\n","pipe_l1 = Pipeline([('poli',PolynomialFeatures(degree = 2)),('escala',StandardScaler()),('classificacao',lr_l1)])\n","pipe_l1.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# L2 regularized logistic regression\n","lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear')\n","pipe_l2 = Pipeline([('poli',PolynomialFeatures(degree = 2)),('escala',StandardScaler()),('classificacao',lr_l2)])\n","pipe_l2.fit(X, y)\n","### END SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score, confusion_matrix,roc_auc_score,roc_curve\n","# f1_score(y,pipe.predict(X))\n","confusion_matrix(y,(pipe_l1.predict_proba(X)[:,0]>0.22087).astype('int'))\n","# roc_auc_score(y_test,pipe_l2.predict_proba(X_test)[:,0],average='weighted')\n","# fpr,tpr,tr = roc_curve(y_test,pipe_l1.predict_proba(X_test)[:,0])\n","# plt.plot(fpr,tpr)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 5\n","\n","*   Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","# Combine all the coefficients into a dataframe\n","coefficients = list()\n","\n","coeff_labels = ['lr', 'l1', 'l2']\n","coeff_models = [lr, lr_l1, lr_l2]\n","\n","for lab,mod in zip(coeff_labels, coeff_models):\n","    coeffs = mod.coef_\n","    coeff_label = pd.MultiIndex(levels=[[lab], [0]], \n","                                  codes=[[0], [0]])\n","    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n","\n","coefficients = pd.concat(coefficients, axis=1)\n","\n","coefficients.index=X_train.columns\n","coefficients"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Prepare six separate plots for each of the multi-class coefficients.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["fig, axList = plt.subplots()\n","fig.set_size_inches(10,5)\n","data = coefficients.xs(0, level=1, axis=1)\n","data.plot(marker='o', ls='-', ms=2.0, ax=axList, legend=False)\n","\n","\n","        \n","plt.xticks(rotation = 90)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 6\n","\n","*   Predict and store the class for each model.\n","*   Store the probability for the predicted class for each model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","# Predict the class and the probability for each\n","y_pred = list()\n","y_prob = list()\n","\n","coeff_labels = ['lr', 'l1', 'l2']\n","coeff_models = [lr, lr_l1, lr_l2]\n","\n","for lab,mod in zip(coeff_labels, coeff_models):\n","    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n","    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n","    \n","y_pred = pd.concat(y_pred, axis=1)\n","y_prob = pd.concat(y_prob, axis=1)\n","\n","y_pred.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["y_prob.head()\n","### END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 7\n","\n","For each model, calculate the following error metrics:\n","\n","*   Accuracy\n","*   Precision\n","*   Recall\n","*   F-score\n","*   Confusion Matrix\n","\n","Decide how to combine the multi-class metrics into a single value for each model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","from sklearn.metrics import precision_recall_fscore_support as score\n","from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n","from sklearn.preprocessing import label_binarize\n","\n","metrics = list()\n","cm = dict()\n","\n","for lab in coeff_labels:\n","\n","    # Preciision, recall, f-score from the multi-class support function\n","    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n","    \n","    # The usual way to calculate accuracy\n","    accuracy = accuracy_score(y_test, y_pred[lab])\n","    \n","    # ROC-AUC scores can be calculated by binarizing the data\n","    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n","              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n","              average='weighted')\n","    \n","    # Last, the confusion matrix\n","    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n","    \n","    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n","                              'fscore':fscore, 'accuracy':accuracy,\n","                              'auc':auc}, \n","                             name=lab))\n","\n","metrics = pd.concat(metrics, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["metrics\n","### END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Question 8\n","\n","*   Display or plot the confusion matrix for each model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","fig, axList = plt.subplots(nrows=2, ncols=2)\n","axList = axList.flatten()\n","fig.set_size_inches(12, 10)\n","\n","axList[-1].axis('off')\n","\n","for ax,lab in zip(axList[:-1], coeff_labels):\n","    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n","    ax.set(title=lab);\n","    \n","plt.tight_layout()\n","### END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["***\n","\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"basico","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
