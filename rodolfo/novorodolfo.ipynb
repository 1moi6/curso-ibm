{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import seaborn as sns, pandas as pd, numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.metrics import make_scorer,f1_score,accuracy_score,confusion_matrix,roc_auc_score,roc_curve, recall_score,precision_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway, kruskal,chi2_contingency\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE \n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours, NeighbourhoodCleaningRule, OneSidedSelection\n",
    "\n",
    "\n",
    "\n",
    "def minha_metrica(y_true, y_pred):\n",
    "    threshold = 0.5  # Defina o threshold desejado\n",
    "    \n",
    "\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= threshold))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < threshold))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred < threshold))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= threshold))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "def geo_score(y_true, y_pred):\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred==1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred==0))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred==0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred==1))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "def minha_metrica_c(y_true, y_pred,threshold = 0.5):\n",
    "    \n",
    "\n",
    "    # Calcule o true positive rate para o threshold dado\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= threshold))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < threshold))\n",
    "    tpr = tp / (tp + fn)\n",
    "\n",
    "    # Calcule o true negative rate para o threshold dado\n",
    "    tn = np.sum((y_true == 0) & (y_pred < threshold))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= threshold))\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    # Calcule o produto dos passos 1 e 2\n",
    "    product = tpr * tnr\n",
    "\n",
    "    # Retorne a raiz quadrada do passo 3\n",
    "    return np.sqrt(product)\n",
    "\n",
    "def get_scores(y_true_tr,y_pred_tr,y_true_ts,y_pred_ts,scorers=[\"roc_auc_score\",\"accuracy_score\",\"precision_score\",\"recall_score\",\"f1_score\"]):\n",
    "    nomes, vtest,vtrain = [],[],[]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_tr, y_pred_tr)\n",
    "    distances = (fpr - 0)**2 + (tpr - 1)**2\n",
    "    index = distances.argmin()\n",
    "    corte = thresholds[index]\n",
    "    fprts, tprts, thresholdsts = roc_curve(y_true_ts, y_pred_ts)\n",
    "\n",
    "    for sc in scorers:\n",
    "        if sc.__name__ in [\"roc_auc_curve\"]:\n",
    "            vtest.append(sc(y_true_ts,y_pred_ts))\n",
    "            vtrain.append(sc(y_true_tr,y_pred_tr))\n",
    "            nomes.append(sc.__name__)\n",
    "        elif sc.__name__ in [\"minha_metrica_c\"]:\n",
    "            vtest.append(sc(y_true_ts, y_pred_ts,threshold = corte))\n",
    "            vtrain.append(sc(y_true_tr, y_pred_tr,threshold = corte))\n",
    "            nomes.append(sc.__name__)\n",
    "        else:\n",
    "            vtest.append(sc(y_true_ts,y_pred_ts>=corte))\n",
    "            vtrain.append(sc(y_true_tr,y_pred_tr>=corte))\n",
    "            nomes.append(sc.__name__)    \n",
    "    \n",
    "    metricas = pd.DataFrame({\"metrica\":nomes,\"valor no treino\":vtrain,\"valor no teste\":vtest})\n",
    "    roc_curve_train = {\"fpr\":fpr,\"tpr\":tpr,\"thresholds\":thresholds,\"corte\":corte} \n",
    "    roc_curve_test = {\"fpr\":fprts,\"tpr\":tprts,\"thresholds\":thresholdsts}\n",
    "    cm1 = confusion_matrix(y_pred=y_pred_ts>=corte,y_true=y_true_ts)\n",
    "    cm2 = confusion_matrix(y_pred=y_pred_ts>=corte,y_true=y_true_ts,normalize='true')\n",
    "    cm = pd.DataFrame({\"pred_0\":[cm1[0][0],cm1[1][0]],\"pred_1\":[cm1[0][1],cm1[1][1]],\"predn_0\":[cm2[0][0],cm2[1][0]],\"predn_1\":[cm2[0][1],cm2[1][1]]},index=[\"true 0\",\"true_1\"])\n",
    "    res = {\"metricas\":metricas,\"roc_curve_train\":roc_curve_train,\"roc_curve_test\":roc_curve_test,\"melhor\":[fpr[index],tpr[index],corte],\"confusion_matrix\":cm}\n",
    "    return res\n",
    "\n",
    "def meu_enconder(data,predictors,target,split = False):\n",
    "    if split:\n",
    "        data_train,data_test = train_test_split(data,test_size=0.2,stratify=data[target],random_state=42)\n",
    "        for cl in predictors:\n",
    "            if data[cl].dtypes==np.object0:\n",
    "                contagemp = data_train.groupby(cl)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "                idx = contagemp.sort_values(by=1,ascending=False).index\n",
    "                mapeamento = {v:i for i,v in enumerate(idx)} \n",
    "                data_train[cl] = data_train[cl].map(mapeamento).astype(int)\n",
    "                data_test[cl] = data_test[cl].map(mapeamento).astype(int)\n",
    "        return data_train,data_test\n",
    "    else:\n",
    "        ndata = data.copy()\n",
    "        for cl in predictors:\n",
    "            if data[cl].dtypes==np.object0:\n",
    "                contagemp = data.groupby(cl)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "                idx = contagemp.sort_values(by=1,ascending=False).index\n",
    "                mapeamento = {v:i for i,v in enumerate(idx)} \n",
    "                ndata[cl] = data[cl].map(mapeamento).astype(int)\n",
    "        return ndata\n",
    "\n",
    "def categorizar(data,columns):\n",
    "    ndata = data.copy()\n",
    "    for cl in columns:\n",
    "        if data[cl].dtypes in [np.int64,np.int32,np.float64]:\n",
    "            categories = pd.qcut(data[cl],10,duplicates='drop')\n",
    "            labels, _ = pd.factorize(categories, sort=True)\n",
    "            ndata[f\"{cl}_cat\"] = labels\n",
    "    return ndata\n",
    "\n",
    "def significancia(data,predictors,target,alpha=0.1):\n",
    "    pval,tval,sigval = [],[],[]\n",
    "    for cl in predictors:\n",
    "        if data[cl].dtypes==np.object0:\n",
    "            contingency_table = pd.crosstab(data[cl], data[target])\n",
    "            chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "            pval.append(p)\n",
    "            tval.append(chi2)\n",
    "            sigval.append(p<alpha)\n",
    "        else:\n",
    "            groups = []\n",
    "            for category, group_data in data.groupby(cl)[target]:\n",
    "                groups.append(group_data)\n",
    "            f_statistic, p = kruskal(*groups)\n",
    "            pval.append(p)\n",
    "            tval.append(f_statistic)\n",
    "            sigval.append(p<alpha)\n",
    "    significantvar = list([predictors[i] for i,v in enumerate(sigval) if v])\n",
    "    stats = pd.DataFrame({\"variable\":predictors,\"test-value\":tval,\"p-value\":pval,\"significance\":sigval})\n",
    "    return {\"stats\":stats,\"significantes\":significantvar}\n",
    "\n",
    "def simulador(estimator,data,predictors,target,nsim,metricas=[geo_score,f1_score,accuracy_score,roc_auc_score]):\n",
    "    metricasval = np.zeros((nsim,len(metricas)))\n",
    "    truepos,trueneg = [],[]\n",
    "    res = {}\n",
    "    for i in range(nsim):\n",
    "        data_train,data_test = train_test_split(data,test_size=0.2, stratify=data[target])\n",
    "        X_train,y_train = data_train[predictors],data_train[target]\n",
    "        X_test,y_test = data_test[predictors],data_test[target]\n",
    "\n",
    "        bests = estimator.fit(X_train,y_train)\n",
    "        y_pred_ts = bests.predict_proba(X_test)[:,1]\n",
    "        y_pred_tr = bests.predict_proba(X_train)[:,1]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_train, y_pred_tr)\n",
    "        distances = (fpr - 0)**2 + (tpr - 1)**2\n",
    "        index = distances.argmin()\n",
    "        corte = thresholds[index]\n",
    "        cm = confusion_matrix(y_pred=y_pred_ts>=corte,y_true=y_test,normalize='true')\n",
    "        truepos.append(cm[1][1])\n",
    "        trueneg.append(cm[0][0])\n",
    "        for j,mtr in enumerate(metricas):\n",
    "            if mtr.__name__ in [\"roc_auc_curve\"]:\n",
    "                metricasval[i,j] = mtr(y_test,y_pred_ts)\n",
    "            else:\n",
    "                metricasval[i,j] = mtr(y_test,y_pred_ts>=corte)\n",
    "    \n",
    "    res[\"tpr\"] = truepos\n",
    "    res[\"tnr\"] = trueneg\n",
    "    for j,mtr in enumerate(metricas):\n",
    "        res[mtr.__name__] = metricasval[:,j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dados_hospital.csv\", sep=',')\n",
    "data.dropna(inplace=True)\n",
    "target = 'no_show'\n",
    "mapeamento = {'SIM': 0, 'NÃO': 1}\n",
    "data[target] = data[target].map(mapeamento).astype(int)\n",
    "data = data[data[\"distance\"]<1000]\n",
    "original_columns = list(data.columns[:-1])\n",
    "\n",
    "categoricalvar = {cl:data[cl].dtypes == np.object0 for cl in original_columns} \n",
    "\n",
    "data = categorizar(data=data,columns=original_columns)\n",
    "data_train,data_test = meu_enconder(data=data,predictors=original_columns,target=target,split=True)\n",
    "\n",
    "res = significancia(data,original_columns,target)\n",
    "predictors = res[\"significantes\"]\n",
    "\n",
    "X_train,y_train = data_train[predictors],data_train[target]\n",
    "X_test,y_test = data_test[predictors],data_test[target]\n",
    "\n",
    "my_scorer,my_scorer_m = make_scorer(roc_auc_score,greater_is_better=True), make_scorer(minha_metrica,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cat = ['age_cat', 'distance_cat', 'lead_time_cat',\n",
    "       'no_cons_scheduled_previous_year_cat', 'no_ns_cons_previous_year_cat',\n",
    "       'no_exa_scheduled_previous_year_cat', 'no_ns_exa_previous_year_cat']\n",
    "for cl in var_cat:\n",
    "    contagemp = data_train.groupby(cl)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "    idx = contagemp.sort_values(by=1,ascending=False).index\n",
    "    mapeamento = {v:i for i,v in enumerate(idx)} \n",
    "    data_train[f\"{cl}_cat2\"] = data_train[cl].map(mapeamento).astype(int)\n",
    "    data_test[f\"{cl}_cat2\"] = data_test[cl].map(mapeamento).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,3)\n",
    "ax = ax.flatten()\n",
    "i = 0\n",
    "for cl in predictors:\n",
    "    sns.histplot(x=cl,data=data_train,ax=ax[i])\n",
    "    i = i+1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,2)\n",
    "ax = ax.flatten()\n",
    "i = 0\n",
    "for cl in predictors:\n",
    "    if categoricalvar[cl]:\n",
    "        contagemp = data_train.groupby(cl)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "        sns.scatterplot(x=cl,y=1,data=contagemp,ax=ax[i])\n",
    "        i = i+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,2)\n",
    "ax = ax.flatten()\n",
    "i = 0\n",
    "for cl in predictors:\n",
    "    if not categoricalvar[cl]:\n",
    "        contagemp = data_train.groupby(f\"{cl}_cat\")[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "        sns.scatterplot(x=f\"{cl}_cat\",y=1,data=contagemp,ax=ax[i],color=\"red\")\n",
    "        sns.lineplot(x=f\"{cl}_cat\",y=1,data=contagemp,ax=ax[i],linestyle='dashed')\n",
    "        i = i+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = \"age_cat\"\n",
    "grau = 2\n",
    "contagemp = data_train.groupby(cls)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "aa = np.polyfit(np.array(contagemp.index),np.array(contagemp[1]),grau)\n",
    "xxp = np.array(contagemp.index)\n",
    "yyp  = np.sum([par*xxp**(grau-i) for i,par in enumerate(aa)],axis=0)\n",
    "sns.scatterplot(x=cls,y=1,data=contagemp,color=\"red\")\n",
    "sns.lineplot(x=cls,y=1,data=contagemp,linestyle='dashed')\n",
    "sns.lineplot(x=xxp,y=yyp)\n",
    "\n",
    "data_train[\"age_2\"] = np.sum([par*data_train[\"age\"]**(grau-i) for i,par in enumerate(aa)],axis=0)\n",
    "data_test[\"age_2\"] = np.sum([par*data_test[\"age\"]**(grau-i) for i,par in enumerate(aa)],axis=0)\n",
    "\n",
    "cls = \"distance_cat\"\n",
    "grau = 2\n",
    "contagemp = data_train.groupby(cls)[target].value_counts(normalize=True).unstack().fillna(0)\n",
    "aa = np.polyfit(np.array(contagemp.index),np.array(contagemp[1]),grau)\n",
    "xxp = np.array(contagemp.index)\n",
    "yyp  = np.sum([par*xxp**(grau-i) for i,par in enumerate(aa)],axis=0)\n",
    "sns.scatterplot(x=cls,y=1,data=contagemp,color=\"red\")\n",
    "sns.lineplot(x=cls,y=1,data=contagemp,linestyle='dashed')\n",
    "sns.lineplot(x=xxp,y=yyp)\n",
    "\n",
    "data_train[\"distance_2\"] = np.sum([par*data_train[\"distance\"]**(grau-i) for i,par in enumerate(aa)],axis=0)\n",
    "data_test[\"distance_2\"] = np.sum([par*data_test[\"distance\"]**(grau-i) for i,par in enumerate(aa)],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('poly',PolynomialFeatures(2)),('escala',MinMaxScaler()),('modelo', LogisticRegression(penalty='l2',class_weight='balanced',C = 1.0e-10,max_iter=1000000))])\n",
    "\n",
    "# pipe = Pipeline([('escala',MinMaxScaler()),('modelo', LogisticRegression(penalty='l2',class_weight='balanced',C = 1.0e-15,max_iter=1000000))])\n",
    "\n",
    "search = cross_validate(pipe, X_train, y_train, scoring=my_scorer, cv=3, return_estimator=True)\n",
    "bests = search['estimator'][np.argmax(search['test_score'])]\n",
    "\n",
    "# bests = pipe.fit(X_train, y_train)\n",
    "\n",
    "# coeficientes = pd.DataFrame({\"variável\":pipe.named_steps[\"poly\"].get_feature_names_out().ravel,\"coeficientes\":bests.named_steps[\"modelo\"].coef_.ravel()})\n",
    "# print(f\"{coeficientes}\\n\")\n",
    "\n",
    "y_pred_test = bests.decision_function(X_test)\n",
    "y_pred_train = bests.decision_function(X_train)\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"modelo__C\" :np.geomspace(1e-8,1e+2,50),\n",
    "    \"poly__degree\" :[1,2,4]\n",
    "}\n",
    "pipe = Pipeline([('poly',PolynomialFeatures()),('escala',MinMaxScaler()),('modelo', LogisticRegression(penalty='l2',class_weight='balanced',max_iter=1000000))])\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid,scoring = my_scorer, cv = 3, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "bests = search.best_estimator_\n",
    "\n",
    "print(\"--melhor escore no cv--\")\n",
    "print(search.best_score_)\n",
    "\n",
    "print(\"\\n--parâmetros de melhor escore no cv--\") \n",
    "print(search.best_params_)\n",
    "\n",
    "# coeficientes = pd.DataFrame({\"variável\":predictors,\"coeficientes\":bests.named_steps[\"modelo\"].coef_})\n",
    "# print(f\"\\n{coeficientes}\")\n",
    "\n",
    "y_pred_test = bests.decision_function(X_test)\n",
    "y_pred_train = bests.decision_function(X_train)\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"\\n{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ga = 'auto'\n",
    "dg = 1\n",
    "\n",
    "# pipe = Pipeline([('escala',MinMaxScaler()),('modelo', SVC(kernel='poly',class_weight='balanced',degree=dg,gamma=ga))])\n",
    "pipe = Pipeline([('escala',MinMaxScaler()),('modelo', SVC(kernel='rbf',class_weight='balanced',degree=dg,gamma=ga))])\n",
    "\n",
    "\n",
    "search = cross_validate(pipe, X_train, y_train, scoring=my_scorer, cv=7, return_estimator=True)\n",
    "bests = search['estimator'][np.argmax(search['test_score'])]\n",
    "\n",
    "y_pred_test = bests.decision_function(X_test)\n",
    "y_pred_train = bests.decision_function(X_train)\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"modelo__gamma\" : list(np.geomspace(1e-5,1e+0,5))+['auto','scale'],\n",
    "    \"modelo__degree\":[1,2],\n",
    "    \"modelo__kernel\": [\"poly\",'rbf']\n",
    "}\n",
    "\n",
    "# pipe = Pipeline([('escala',MinMaxScaler()),('modelo', SVC(kernel='poly',class_weight='balanced'))])\n",
    "\n",
    "pipe = Pipeline([('escala',MinMaxScaler()),('modelo', SVC(class_weight='balanced'))])\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, scoring = my_scorer, cv = 5, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "bests = search.best_estimator_\n",
    "\n",
    "y_pred_test = bests.decision_function(X_test)\n",
    "y_pred_train = bests.decision_function(X_train)\n",
    "\n",
    "print(f\"melhor score cv:\\n{search.best_score_}\\n\")\n",
    "print(f\"parâmetros melhor score cv:\\n{search.best_params_}\\n\")\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"melhor score cv:\\n{search.best_score_}\\n\")\n",
    "print(f\"parâmetros melhor score cv:\\n{search.best_params_}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 5\n",
    "max_features = \"log2\"\n",
    "max_depth = 2\n",
    "\n",
    "pipe = Pipeline([('modelo', RandomForestClassifier(n_estimators=n_estimators,max_features=max_features,class_weight='balanced',max_depth=max_depth))])\n",
    "\n",
    "search = cross_validate(pipe, X_train, y_train, scoring=my_scorer, cv=7, return_estimator=True)\n",
    "bests = search['estimator'][np.argmax(search['test_score'])]\n",
    "\n",
    "y_pred_test = bests.predict_proba(X_test)[:,1]\n",
    "y_pred_train = bests.predict_proba(X_train)[:,1]\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'modelo__n_estimators': [2*n+1 for n in range(2,10)],\n",
    "    'modelo__max_features':[\"sqrt\", \"log2\"],\n",
    "    'modelo__max_depth': [2,3,4,5,7]\n",
    "    }\n",
    "\n",
    "pipe = Pipeline([('modelo', RandomForestClassifier(class_weight='balanced'))])\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, scoring = my_scorer, cv = 5, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "bests = search.best_estimator_\n",
    "\n",
    "print(f\"melhor score cv:\\n{search.best_score_}\\n\")\n",
    "print(f\"parâmetros melhor score cv:\\n{search.best_params_}\\n\")\n",
    "\n",
    "y_pred_test = bests.predict_proba(X_test)[:,1]\n",
    "y_pred_train = bests.predict_proba(X_train)[:,1]\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'modelo__n_estimators': [2*n+1 for n in range(2,50)],\n",
    "    'modelo__max_features':[\"sqrt\", \"log2\"],\n",
    "    'modelo__max_depth': [2,3,4]\n",
    "    }\n",
    "\n",
    "pipe = Pipeline([('modelo', RandomForestClassifier(class_weight='balanced'))])\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, scoring = my_scorer, cv = 5, n_jobs=-1)\n",
    "\n",
    "sampler = RandomOverSampler()\n",
    "X_trans,y_trans = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "search.fit(X_trans, y_trans)\n",
    "bests = search.best_estimator_\n",
    "\n",
    "print(f\"melhor score cv:\\n{search.best_score_}\\n\")\n",
    "print(f\"parâmetros melhor score cv:\\n{search.best_params_}\\n\")\n",
    "\n",
    "y_pred_test = bests.predict_proba(X_test)[:,1]\n",
    "y_pred_train = bests.predict_proba(X_train)[:,1]\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 55\n",
    "max_features = \"log2\"\n",
    "max_depth = 3\n",
    "criterion = 'log_loss'\n",
    "\n",
    "pipe = Pipeline([('modelo', RandomForestClassifier(criterion=criterion,n_estimators=n_estimators,max_features=max_features,class_weight='balanced',max_depth=max_depth))])\n",
    "\n",
    "sampler = SMOTE()\n",
    "X_trans,y_trans = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "search = cross_validate(pipe, X_trans, y_trans, scoring=my_scorer_m, cv=5, return_estimator=True,return_train_score=True)\n",
    "bests = search['estimator'][np.argmax(search['test_score'])]\n",
    "\n",
    "y_pred_test = bests.predict_proba(X_test)[:,1]\n",
    "y_pred_train = bests.predict_proba(X_train)[:,1]\n",
    "\n",
    "\n",
    "res = get_scores(y_train,y_pred_train,y_test,y_pred_test,scorers=[minha_metrica_c,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score])\n",
    "print(f\"{res['metricas']}\\n\")\n",
    "print(f\"{res['confusion_matrix']}\\n\")\n",
    "curva_test = res[\"roc_curve_test\"]\n",
    "curva_train = res[\"roc_curve_train\"]\n",
    "melhor = res[\"melhor\"]\n",
    "plt.plot(curva_train[\"fpr\"], curva_train[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot(curva_test[\"fpr\"], curva_test[\"tpr\"], label='ROC Curve - train')\n",
    "plt.plot([0, 1], [0, 1], color = 'k', ls = 'dashed', label='Random classifier')\n",
    "plt.scatter(melhor[0],melhor[1],label='Best Classifier',color = 'red')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dados_hospital.csv\", sep=',')\n",
    "data.dropna(inplace=True)\n",
    "target = 'no_show'\n",
    "mapeamento = {'SIM': 0, 'NÃO': 1}\n",
    "data[target] = data[target].map(mapeamento).astype(int)\n",
    "data = data[data[\"distance\"]<1000]\n",
    "original_columns = list(data.columns[:-1])\n",
    "categoricalvar = {cl:data[cl].dtypes == np.object0 for cl in original_columns} \n",
    "data = categorizar(data=data,columns=original_columns)\n",
    "data = meu_enconder(data=data,predictors=original_columns,target=target,split=False)\n",
    "res = significancia(data,original_columns,target)\n",
    "predictors = res[\"significantes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim = 100\n",
    "n_estimators = 55\n",
    "max_features = \"log2\"\n",
    "max_depth = 3\n",
    "criterion = 'log_loss'\n",
    "\n",
    "pipe = Pipeline([('modelo', RandomForestClassifier(criterion=criterion,n_estimators=n_estimators,max_features=max_features,class_weight='balanced',max_depth=max_depth))])\n",
    "res = simulador(pipe,data,predictors,target,nsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo 0 de 15\n",
      "modelo 1 de 15\n",
      "modelo 2 de 15\n",
      "modelo 3 de 15\n",
      "modelo 4 de 15\n",
      "modelo 5 de 15\n",
      "modelo 6 de 15\n",
      "modelo 7 de 15\n",
      "modelo 8 de 15\n",
      "modelo 9 de 15\n",
      "modelo 10 de 15\n",
      "modelo 11 de 15\n",
      "modelo 12 de 15\n",
      "modelo 13 de 15\n",
      "modelo 14 de 15\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "n_est = [15,25,55,75,99]\n",
    "max_features = [\"log2\"]\n",
    "max_depth = [3,5,6]\n",
    "crit = ['log_loss']\n",
    "\n",
    "combinations = list(product(n_est, max_depth,max_features,crit))\n",
    "\n",
    "RES = []\n",
    "for k,v in enumerate(combinations):\n",
    "    print(f\"modelo {k+1} de {len(combinations)}\")\n",
    "    pipe = Pipeline([('modelo', RandomForestClassifier(criterion=v[3],n_estimators=v[0],max_features=v[2],class_weight='balanced',max_depth=v[1]))])\n",
    "    res = simulador(pipe,data,predictors,target,50)\n",
    "    RES.append(res[\"geo_score\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_statistic, p_value = f_oneway(*RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import tukey_hsd\n",
    "resf = tukey_hsd(*RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\n",
      "Comparison  Statistic  p-value  Lower CI  Upper CI\n",
      " (0 - 1)      0.007     0.991    -0.011     0.025\n",
      " (0 - 2)      0.020     0.011     0.002     0.038\n",
      " (0 - 3)     -0.001     1.000    -0.019     0.016\n",
      " (0 - 4)      0.010     0.804    -0.007     0.028\n",
      " (0 - 5)      0.028     0.000     0.010     0.046\n",
      " (0 - 6)     -0.002     1.000    -0.019     0.016\n",
      " (0 - 7)      0.007     0.984    -0.010     0.025\n",
      " (0 - 8)      0.018     0.037     0.000     0.036\n",
      " (0 - 9)      0.002     1.000    -0.016     0.020\n",
      " (0 - 10)      0.013     0.413    -0.004     0.031\n",
      " (0 - 11)      0.020     0.013     0.002     0.037\n",
      " (0 - 12)      0.002     1.000    -0.016     0.019\n",
      " (0 - 13)      0.007     0.990    -0.011     0.025\n",
      " (0 - 14)      0.016     0.108    -0.001     0.034\n",
      " (1 - 0)     -0.007     0.991    -0.025     0.011\n",
      " (1 - 2)      0.013     0.434    -0.005     0.031\n",
      " (1 - 3)     -0.008     0.967    -0.026     0.010\n",
      " (1 - 4)      0.003     1.000    -0.014     0.021\n",
      " (1 - 5)      0.021     0.005     0.003     0.039\n",
      " (1 - 6)     -0.009     0.941    -0.026     0.009\n",
      " (1 - 7)      0.000     1.000    -0.017     0.018\n",
      " (1 - 8)      0.011     0.700    -0.006     0.029\n",
      " (1 - 9)     -0.005     1.000    -0.022     0.013\n",
      " (1 - 10)      0.006     0.997    -0.011     0.024\n",
      " (1 - 11)      0.013     0.480    -0.005     0.030\n",
      " (1 - 12)     -0.005     0.999    -0.023     0.012\n",
      " (1 - 13)      0.000     1.000    -0.018     0.018\n",
      " (1 - 14)      0.009     0.900    -0.008     0.027\n",
      " (2 - 0)     -0.020     0.011    -0.038    -0.002\n",
      " (2 - 1)     -0.013     0.434    -0.031     0.005\n",
      " (2 - 3)     -0.021     0.005    -0.039    -0.003\n",
      " (2 - 4)     -0.010     0.870    -0.027     0.008\n",
      " (2 - 5)      0.008     0.967    -0.010     0.026\n",
      " (2 - 6)     -0.022     0.003    -0.039    -0.004\n",
      " (2 - 7)     -0.013     0.503    -0.030     0.005\n",
      " (2 - 8)     -0.002     1.000    -0.019     0.016\n",
      " (2 - 9)     -0.018     0.043    -0.035    -0.000\n",
      " (2 - 10)     -0.007     0.993    -0.024     0.011\n",
      " (2 - 11)     -0.000     1.000    -0.018     0.017\n",
      " (2 - 12)     -0.018     0.032    -0.036    -0.001\n",
      " (2 - 13)     -0.013     0.446    -0.031     0.005\n",
      " (2 - 14)     -0.004     1.000    -0.021     0.014\n",
      " (3 - 0)      0.001     1.000    -0.016     0.019\n",
      " (3 - 1)      0.008     0.967    -0.010     0.026\n",
      " (3 - 2)      0.021     0.005     0.003     0.039\n",
      " (3 - 4)      0.011     0.662    -0.006     0.029\n",
      " (3 - 5)      0.029     0.000     0.011     0.047\n",
      " (3 - 6)     -0.001     1.000    -0.018     0.017\n",
      " (3 - 7)      0.009     0.946    -0.009     0.026\n",
      " (3 - 8)      0.019     0.018     0.002     0.037\n",
      " (3 - 9)      0.003     1.000    -0.014     0.021\n",
      " (3 - 10)      0.014     0.275    -0.003     0.032\n",
      " (3 - 11)      0.021     0.006     0.003     0.038\n",
      " (3 - 12)      0.003     1.000    -0.015     0.020\n",
      " (3 - 13)      0.008     0.963    -0.009     0.026\n",
      " (3 - 14)      0.017     0.058    -0.000     0.035\n",
      " (4 - 0)     -0.010     0.804    -0.028     0.007\n",
      " (4 - 1)     -0.003     1.000    -0.021     0.014\n",
      " (4 - 2)      0.010     0.870    -0.008     0.027\n",
      " (4 - 3)     -0.011     0.662    -0.029     0.006\n",
      " (4 - 5)      0.018     0.047     0.000     0.035\n",
      " (4 - 6)     -0.012     0.577    -0.030     0.006\n",
      " (4 - 7)     -0.003     1.000    -0.020     0.015\n",
      " (4 - 8)      0.008     0.975    -0.010     0.025\n",
      " (4 - 9)     -0.008     0.961    -0.026     0.009\n",
      " (4 - 10)      0.003     1.000    -0.015     0.020\n",
      " (4 - 11)      0.009     0.897    -0.008     0.027\n",
      " (4 - 12)     -0.009     0.940    -0.026     0.009\n",
      " (4 - 13)     -0.003     1.000    -0.021     0.014\n",
      " (4 - 14)      0.006     0.998    -0.012     0.024\n",
      " (5 - 0)     -0.028     0.000    -0.046    -0.010\n",
      " (5 - 1)     -0.021     0.005    -0.039    -0.003\n",
      " (5 - 2)     -0.008     0.967    -0.026     0.010\n",
      " (5 - 3)     -0.029     0.000    -0.047    -0.011\n",
      " (5 - 4)     -0.018     0.047    -0.035    -0.000\n",
      " (5 - 6)     -0.030     0.000    -0.047    -0.012\n",
      " (5 - 7)     -0.021     0.007    -0.038    -0.003\n",
      " (5 - 8)     -0.010     0.845    -0.028     0.008\n",
      " (5 - 9)     -0.026     0.000    -0.044    -0.008\n",
      " (5 - 10)     -0.015     0.212    -0.032     0.003\n",
      " (5 - 11)     -0.008     0.954    -0.026     0.009\n",
      " (5 - 12)     -0.026     0.000    -0.044    -0.009\n",
      " (5 - 13)     -0.021     0.005    -0.039    -0.003\n",
      " (5 - 14)     -0.012     0.613    -0.029     0.006\n",
      " (6 - 0)      0.002     1.000    -0.016     0.019\n",
      " (6 - 1)      0.009     0.941    -0.009     0.026\n",
      " (6 - 2)      0.022     0.003     0.004     0.039\n",
      " (6 - 3)      0.001     1.000    -0.017     0.018\n",
      " (6 - 4)      0.012     0.577    -0.006     0.030\n",
      " (6 - 5)      0.030     0.000     0.012     0.047\n",
      " (6 - 7)      0.009     0.911    -0.008     0.027\n",
      " (6 - 8)      0.020     0.012     0.002     0.037\n",
      " (6 - 9)      0.004     1.000    -0.014     0.021\n",
      " (6 - 10)      0.015     0.214    -0.003     0.032\n",
      " (6 - 11)      0.021     0.004     0.004     0.039\n",
      " (6 - 12)      0.003     1.000    -0.014     0.021\n",
      " (6 - 13)      0.009     0.936    -0.009     0.026\n",
      " (6 - 14)      0.018     0.041     0.000     0.036\n",
      " (7 - 0)     -0.007     0.984    -0.025     0.010\n",
      " (7 - 1)     -0.000     1.000    -0.018     0.017\n",
      " (7 - 2)      0.013     0.503    -0.005     0.030\n",
      " (7 - 3)     -0.009     0.946    -0.026     0.009\n",
      " (7 - 4)      0.003     1.000    -0.015     0.020\n",
      " (7 - 5)      0.021     0.007     0.003     0.038\n",
      " (7 - 6)     -0.009     0.911    -0.027     0.008\n",
      " (7 - 8)      0.011     0.762    -0.007     0.028\n",
      " (7 - 9)     -0.005     0.999    -0.023     0.012\n",
      " (7 - 10)      0.006     0.999    -0.012     0.023\n",
      " (7 - 11)      0.012     0.549    -0.005     0.030\n",
      " (7 - 12)     -0.006     0.999    -0.023     0.012\n",
      " (7 - 13)     -0.000     1.000    -0.018     0.017\n",
      " (7 - 14)      0.009     0.932    -0.009     0.026\n",
      " (8 - 0)     -0.018     0.037    -0.036    -0.000\n",
      " (8 - 1)     -0.011     0.700    -0.029     0.006\n",
      " (8 - 2)      0.002     1.000    -0.016     0.019\n",
      " (8 - 3)     -0.019     0.018    -0.037    -0.002\n",
      " (8 - 4)     -0.008     0.975    -0.025     0.010\n",
      " (8 - 5)      0.010     0.845    -0.008     0.028\n",
      " (8 - 6)     -0.020     0.012    -0.037    -0.002\n",
      " (8 - 7)     -0.011     0.762    -0.028     0.007\n",
      " (8 - 9)     -0.016     0.122    -0.034     0.002\n",
      " (8 - 10)     -0.005     1.000    -0.023     0.013\n",
      " (8 - 11)      0.002     1.000    -0.016     0.019\n",
      " (8 - 12)     -0.016     0.097    -0.034     0.001\n",
      " (8 - 13)     -0.011     0.712    -0.029     0.007\n",
      " (8 - 14)     -0.002     1.000    -0.019     0.016\n",
      " (9 - 0)     -0.002     1.000    -0.020     0.016\n",
      " (9 - 1)      0.005     1.000    -0.013     0.022\n",
      " (9 - 2)      0.018     0.043     0.000     0.035\n",
      " (9 - 3)     -0.003     1.000    -0.021     0.014\n",
      " (9 - 4)      0.008     0.961    -0.009     0.026\n",
      " (9 - 5)      0.026     0.000     0.008     0.044\n",
      " (9 - 6)     -0.004     1.000    -0.021     0.014\n",
      " (9 - 7)      0.005     0.999    -0.012     0.023\n",
      " (9 - 8)      0.016     0.122    -0.002     0.034\n",
      " (9 - 10)      0.011     0.709    -0.007     0.029\n",
      " (9 - 11)      0.018     0.052    -0.000     0.035\n",
      " (9 - 12)     -0.000     1.000    -0.018     0.017\n",
      " (9 - 13)      0.005     1.000    -0.013     0.023\n",
      " (9 - 14)      0.014     0.284    -0.003     0.032\n",
      " (10 - 0)     -0.013     0.413    -0.031     0.004\n",
      " (10 - 1)     -0.006     0.997    -0.024     0.011\n",
      " (10 - 2)      0.007     0.993    -0.011     0.024\n",
      " (10 - 3)     -0.014     0.275    -0.032     0.003\n",
      " (10 - 4)     -0.003     1.000    -0.020     0.015\n",
      " (10 - 5)      0.015     0.212    -0.003     0.032\n",
      " (10 - 6)     -0.015     0.214    -0.032     0.003\n",
      " (10 - 7)     -0.006     0.999    -0.023     0.012\n",
      " (10 - 8)      0.005     1.000    -0.013     0.023\n",
      " (10 - 9)     -0.011     0.709    -0.029     0.007\n",
      " (10 - 11)      0.006     0.996    -0.011     0.024\n",
      " (10 - 12)     -0.012     0.648    -0.029     0.006\n",
      " (10 - 13)     -0.006     0.998    -0.024     0.012\n",
      " (10 - 14)      0.003     1.000    -0.014     0.021\n",
      " (11 - 0)     -0.020     0.013    -0.037    -0.002\n",
      " (11 - 1)     -0.013     0.480    -0.030     0.005\n",
      " (11 - 2)      0.000     1.000    -0.017     0.018\n",
      " (11 - 3)     -0.021     0.006    -0.038    -0.003\n",
      " (11 - 4)     -0.009     0.897    -0.027     0.008\n",
      " (11 - 5)      0.008     0.954    -0.009     0.026\n",
      " (11 - 6)     -0.021     0.004    -0.039    -0.004\n",
      " (11 - 7)     -0.012     0.549    -0.030     0.005\n",
      " (11 - 8)     -0.002     1.000    -0.019     0.016\n",
      " (11 - 9)     -0.018     0.052    -0.035     0.000\n",
      " (11 - 10)     -0.006     0.996    -0.024     0.011\n",
      " (11 - 12)     -0.018     0.040    -0.036    -0.000\n",
      " (11 - 13)     -0.013     0.492    -0.030     0.005\n",
      " (11 - 14)     -0.003     1.000    -0.021     0.014\n",
      " (12 - 0)     -0.002     1.000    -0.019     0.016\n",
      " (12 - 1)      0.005     0.999    -0.012     0.023\n",
      " (12 - 2)      0.018     0.032     0.001     0.036\n",
      " (12 - 3)     -0.003     1.000    -0.020     0.015\n",
      " (12 - 4)      0.009     0.940    -0.009     0.026\n",
      " (12 - 5)      0.026     0.000     0.009     0.044\n",
      " (12 - 6)     -0.003     1.000    -0.021     0.014\n",
      " (12 - 7)      0.006     0.999    -0.012     0.023\n",
      " (12 - 8)      0.016     0.097    -0.001     0.034\n",
      " (12 - 9)      0.000     1.000    -0.017     0.018\n",
      " (12 - 10)      0.012     0.648    -0.006     0.029\n",
      " (12 - 11)      0.018     0.040     0.000     0.036\n",
      " (12 - 13)      0.005     0.999    -0.012     0.023\n",
      " (12 - 14)      0.015     0.236    -0.003     0.032\n",
      " (13 - 0)     -0.007     0.990    -0.025     0.011\n",
      " (13 - 1)     -0.000     1.000    -0.018     0.018\n",
      " (13 - 2)      0.013     0.446    -0.005     0.031\n",
      " (13 - 3)     -0.008     0.963    -0.026     0.009\n",
      " (13 - 4)      0.003     1.000    -0.014     0.021\n",
      " (13 - 5)      0.021     0.005     0.003     0.039\n",
      " (13 - 6)     -0.009     0.936    -0.026     0.009\n",
      " (13 - 7)      0.000     1.000    -0.017     0.018\n",
      " (13 - 8)      0.011     0.712    -0.007     0.029\n",
      " (13 - 9)     -0.005     1.000    -0.023     0.013\n",
      " (13 - 10)      0.006     0.998    -0.012     0.024\n",
      " (13 - 11)      0.013     0.492    -0.005     0.030\n",
      " (13 - 12)     -0.005     0.999    -0.023     0.012\n",
      " (13 - 14)      0.009     0.906    -0.008     0.027\n",
      " (14 - 0)     -0.016     0.108    -0.034     0.001\n",
      " (14 - 1)     -0.009     0.900    -0.027     0.008\n",
      " (14 - 2)      0.004     1.000    -0.014     0.021\n",
      " (14 - 3)     -0.017     0.058    -0.035     0.000\n",
      " (14 - 4)     -0.006     0.998    -0.024     0.012\n",
      " (14 - 5)      0.012     0.613    -0.006     0.029\n",
      " (14 - 6)     -0.018     0.041    -0.036    -0.000\n",
      " (14 - 7)     -0.009     0.932    -0.026     0.009\n",
      " (14 - 8)      0.002     1.000    -0.016     0.019\n",
      " (14 - 9)     -0.014     0.284    -0.032     0.003\n",
      " (14 - 10)     -0.003     1.000    -0.021     0.014\n",
      " (14 - 11)      0.003     1.000    -0.014     0.021\n",
      " (14 - 12)     -0.015     0.236    -0.032     0.003\n",
      " (14 - 13)     -0.009     0.906    -0.027     0.008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Contrast\n",
      "2\n",
      "A\n",
      "3\n",
      "B\n",
      "4\n",
      "Paired\n",
      "5\n",
      "Parametric\n",
      "6\n",
      "T\n",
      "7\n",
      "dof\n",
      "8\n",
      "alternative\n",
      "9\n",
      "p-unc\n",
      "10\n",
      "p-corr\n",
      "11\n",
      "p-adjust\n",
      "12\n",
      "BF10\n",
      "13\n",
      "hedges\n"
     ]
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "# Reshape os dados para o formato longo\n",
    "df_long = amostras.melt(var_name='group', value_name='value')\n",
    "\n",
    "# Realize o teste de Dunn-Bonferroni\n",
    "posthoc = pg.pairwise_ttests(data=df_long, dv='value', between='group', padjust='bonf')\n",
    "\n",
    "alpha = 0.05\n",
    "significant_comparisons = posthoc[posthoc['p-unc'] < alpha]\n",
    "\n",
    "i=0\n",
    "# Print significant group comparisons\n",
    "for comparison in significant_comparisons:\n",
    "    i = i+1\n",
    "    print(i)\n",
    "    print(comparison)\n",
    "# .itertuples():\n",
    "#     group1 = comparison.Index[0]\n",
    "#     group2 = comparison.Index[1]\n",
    "#     print(f\"Significant difference between {group1} and {group2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posthoc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
